{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Generative v/s Discriminative Models\n",
    "\n",
    "### Generative Models\n",
    "\n",
    "##### 1. Naive Bayes' Classifier\n",
    "Naive Bayes (NB) is generative because it captures P(x|y) and P(y), and thus you have P(x,y). Therefore, it captures the distribution of each class. For each class, NB gives the probability of generating the given input by that class. It also assumes that the features are conditionally independent. \n",
    "\n",
    "##### 2. GAN\n",
    "Generative Adversarial Network pose the training process as a game between two separate networks: a generator network  and a second discriminative network that tries to classify samples as either coming from the true distribution p(x) or the model distribution ^p(x). Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away, until at the end (in theory) the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference.\n",
    "\n",
    "##### 3. Gaussian Mixture Model  \n",
    "It is a generative model for data clustering. Data is assumed to be generated from a mixture of K Gaussians. To generate a data point, first select the Gaussian distribution from K distributions using a prior probability. Then using the gaussian distribution, generate the data point. Therefore, it models the data distribution.\n",
    "\n",
    "##### 4. Latent Dirichlet Allocation  \n",
    "LDA discovers topics from sentence(s). It represents documents as mixtures of topics that spit out words with certain probabilities. Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection. Therefore, it models the joint distribution P(x,y) similar to HMM.\n",
    "\n",
    "### Discriminative Models\n",
    "\n",
    "##### 1. Logistic Regression  \n",
    "Partition function Z is a normalization factor. Dividing product of factors by Z gives the conditional probability of P(y|x). Hence logistic regression is discriminative. It is similar to NB but it makes a prediction for the probability using a direct functional form where as Naive Bayes figures out how the data was generated given the results.\n",
    "\n",
    "##### 2. SVM  \n",
    "SVM is a maximal margin classifier which <i>learns the <b>hyperplane</b> (decision boundary)</i> separating two classes. It does not model the distribution of individual classes. It never knows what each class is, it just knows how to tell it apart from the other class.\n",
    "\n",
    "##### 3. Neural Networks  \n",
    "NNs do not model class-distribution. Input is fed into the network and the output layer gives the probability of the input belonging to each class. The likelihood and log-likelihood objective functions are both equivalent to the probability distribution p(y|x) as follows:\n",
    "\n",
    "                                     L(θ) = L(θ;X,y) = p(y|X,θ) \n",
    "##### 4. Decision Trees\n",
    "Similar to SVMs, Decision Trees also explicitly learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = brown.tagged_sents(tagset='universal')[:-10]\n",
    "test_data= brown.tagged_sents(tagset='universal')[-10:]\n",
    "\n",
    "start_mat = {}\n",
    "transition_mat={}\n",
    "emission_mat={}\n",
    "\n",
    "all_words = set()\n",
    "\n",
    "for sent in corpus:\n",
    "    if sent[0][1] not in start_mat:\n",
    "        start_mat[sent[0][1]] = 0\n",
    "    start_mat[sent[0][1]] += 1\n",
    "    for i in range(len(sent)):\n",
    "        elem = sent[i]\n",
    "        w = elem[0].lower()\n",
    "        tag= elem[1]\n",
    "        \n",
    "        all_words.add(w)\n",
    "\n",
    "        if tag not in emission_mat:\n",
    "            emission_mat[tag]= {w:1}\n",
    "        elif w not in emission_mat[tag]:\n",
    "            emission_mat[tag][w] = 1\n",
    "        else:\n",
    "            emission_mat[tag][w] += 1\n",
    "\n",
    "        if i == len(sent)-1:\n",
    "            next_state = 'stop'\n",
    "        else:\n",
    "            next_state = sent[i+1][1]\n",
    "            \n",
    "        if tag not in transition_mat:\n",
    "            transition_mat[tag] = {next_state : 1}\n",
    "        elif next_state not in transition_mat[tag]:\n",
    "            transition_mat[tag][next_state] = 1\n",
    "        else:\n",
    "            transition_mat[tag][next_state] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = sum(start_mat.values())            \n",
    "for state in start_mat:\n",
    "    start_mat[state] /= total\n",
    "    \n",
    "# Laplacian smoothing\n",
    "k = 0.000001\n",
    "total_word_count = len(all_words)\n",
    "\n",
    "for state in emission_mat:\n",
    "    total = sum(emission_mat[state].values())\n",
    "    for w in emission_mat[state]:\n",
    "        emission_mat[state][w] = (emission_mat[state][w]+k)/(total+ k*total_word_count)\n",
    "    emission_mat[state]['unknown word'] = k/(total+ k*total_word_count)    \n",
    "        \n",
    "for state in transition_mat:\n",
    "    total = sum(transition_mat[state].values())\n",
    "    for next_state in transition_mat[state]:\n",
    "        transition_mat[state][next_state] = (transition_mat[state][next_state] + k)/(total + k*total_word_count)\n",
    "    transition_mat[state]['unknown transition'] = k/(total+ k*total_word_count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_viterbi(V,new_state,observation):\n",
    "    best = (0,new_state)\n",
    "    base_prob = 0.00000001\n",
    "    for old_state in V:\n",
    "        if observation not in emission_mat[new_state]:\n",
    "            if new_state not in transition_mat[old_state]:\n",
    "                prob = V[old_state]*transition_mat[old_state]['unknown transition']*emission_mat[new_state]['unknown word']\n",
    "            else:\n",
    "                prob = V[old_state]*transition_mat[old_state][new_state]*emission_mat[new_state]['unknown word']\n",
    "        else:\n",
    "            if new_state not in transition_mat[old_state]:\n",
    "                prob = V[old_state]*emission_mat[new_state][observation]*transition_mat[old_state]['unknown transition']\n",
    "            else:\n",
    "                prob = V[old_state]*transition_mat[old_state][new_state]*emission_mat[new_state][observation]\n",
    "        if prob > best[0]:\n",
    "            best = (prob,old_state)\n",
    "    return best            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sent(sent):\n",
    "    tokens = [i.lower() for i in sent.split()]\n",
    "    \n",
    "    # initialize\n",
    "    V = dict()\n",
    "    B = list()\n",
    "    for state in emission_mat:\n",
    "        try:\n",
    "            V[state] = (start_mat[state]*emission_mat[state][tokens[0]])\n",
    "        except:\n",
    "            V[state] = start_mat[state]*emission_mat[state]['unknown word']\n",
    "    \n",
    "    # recurse\n",
    "    for token in tokens[1:]:\n",
    "        V_updated = dict()\n",
    "        B.append({})\n",
    "        for state in emission_mat:\n",
    "            V_updated[state], B[-1][state] = update_viterbi(V,state,token)\n",
    "        V = V_updated\n",
    "    \n",
    "    # terminate\n",
    "    best = (0,'.')\n",
    "    for state in V:\n",
    "        prob = V[state]*transition_mat[state]['stop']\n",
    "        if prob>best[0]:\n",
    "            best = (prob,state)\n",
    "    \n",
    "    # back-track\n",
    "    current_state = best[1]\n",
    "    pos = [current_state]\n",
    "    B.reverse()\n",
    "    for b in B:\n",
    "        current_state = b[current_state]\n",
    "        pos.append(current_state)\n",
    "    pos.reverse()\n",
    "    \n",
    "    pos_seq = list(zip(tokens,pos))\n",
    "    return pos_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "    diff_tokens = 0\n",
    "    result = {}\n",
    "    for sent in test_data:\n",
    "        actual_sent = ' '.join([t[0] for t in sent])\n",
    "        pos_seq = tag_sent(actual_sent)\n",
    "        for model_tag, actual_tag in zip(pos_seq,sent):\n",
    "            if(model_tag[0] == actual_tag[0].lower()):\n",
    "                if model_tag[1] not in result:\n",
    "                    result[model_tag[1]] = {'actual_count':0, 'correct':0, 'incorrect':0}\n",
    "                if actual_tag[1] not in result:\n",
    "                    result[actual_tag[1]] = {'actual_count':0, 'correct':0, 'incorrect':0}    \n",
    "                if (model_tag[1]==actual_tag[1]):\n",
    "                    correct_tags += 1\n",
    "                    result[model_tag[1]]['actual_count'] += 1\n",
    "                    result[model_tag[1]]['correct'] += 1   \n",
    "                else:\n",
    "                    result[actual_tag[1]]['actual_count'] += 1\n",
    "                    result[model_tag[1]]['incorrect'] += 1\n",
    "            else:\n",
    "                diff_tokens += 1\n",
    "        total_tags += len(sent)\n",
    "    acc = correct_tags/total_tags\n",
    "    for tag in result:\n",
    "        result[tag]['precision'] = result[tag]['correct']/(result[tag]['correct']+result[tag]['incorrect'])\n",
    "        result[tag]['recall'] = result[tag]['correct']/result[tag]['actual_count']\n",
    "        \n",
    "    return acc, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty(result):\n",
    "    prec, recl = [],[]\n",
    "    d = result[1]\n",
    "    print('\\033[1mPOS\\tPrecision Recall\\033[0m')\n",
    "    for key, value in d.items():\n",
    "        print('{}\\t{:0.3f}\\t{:0.3f}'.format(key,value['precision'],value['recall']))\n",
    "        prec.append(value['precision'])\n",
    "        recl.append(value['recall'])\n",
    "    print('\\033[1mAvg:\\t{:0.3f}\\t{:0.3f}\\033[0m'.format(sum(prec)/len(prec),sum(recl)/len(recl)))   \n",
    "    print('\\033[1mAccuracy : {:0.3f}\\033[0m'.format(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPOS\tPrecision Recall\u001b[0m\n",
      "PRON\t1.000\t1.000\n",
      "VERB\t0.941\t0.914\n",
      "ADV\t0.818\t1.000\n",
      "PRT\t0.917\t1.000\n",
      "ADP\t1.000\t0.963\n",
      "NOUN\t0.957\t0.863\n",
      "DET\t1.000\t1.000\n",
      "CONJ\t1.000\t1.000\n",
      "ADJ\t0.933\t0.778\n",
      ".\t0.943\t1.000\n",
      "X\t0.250\t0.667\n",
      "\u001b[1mAvg:\t0.887\t0.926\u001b[0m\n",
      "\u001b[1mAccuracy : 0.933\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Conditional Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "train_sents= corpus\n",
    "\n",
    "def word2features(sent,i):\n",
    "    word = sent[i][0]\n",
    "    tag = sent[i][1]\n",
    "    \n",
    "    return {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'is_first': i == 0,\n",
    "        'is_last': i == len(sent) - 1,\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'is_all_caps': word.upper() == word,\n",
    "        'is_all_lower': word.lower() == word,\n",
    "        'prefix-1': word[0],\n",
    "        'prefix-2': word[:2],\n",
    "        'prefix-3': word[:3],\n",
    "        'suffix-1': word[-1],\n",
    "        'suffix-2': word[-2:],\n",
    "        'suffix-3': word[-3:],\n",
    "        'prev_word': '' if i == 0 else sent[i - 1][0],\n",
    "        'next_word': '' if i == len(sent) - 1 else sent[i + 1][0],\n",
    "        'prev_tag': '' if i == 0 else sent[i - 1][1],\n",
    "        'curr_tag': tag,\n",
    "        'next_tag': '' if i == len(sent) - 1 else sent[i + 1][1],\n",
    "        'has_hyphen': '-' in word,\n",
    "        'is_numeric': word.isdigit(),\n",
    "        'capitals_inside': word[1:].lower() != word[1:],\n",
    "        'length of word': len(word),\n",
    "        'adverb': 1 if(word.endswith('ly') and tag=='ADV') else 0,\n",
    "        'verb': 1 if(i==0 and tag=='VERB' and sent[-1][0]=='?') else 0,\n",
    "        'adjective': 1 if(i>0 and tag=='NOUN' and sent[i-1][1]=='ADJ') else 0,\n",
    "        'noun': 1 if(word[0].isupper() and tag=='NOUN') else 0,\n",
    "        'number': 1 if(word[0].isdigit() and tag=='NUM') else 0,        \n",
    "    }\n",
    "                \n",
    "def sent2features(sent):\n",
    "    return [word2features(sent,i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for i,label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=[sent2features(s) for s in train_sents]\n",
    "y_train=[sent2labels(s) for s in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=[sent2features(s) for s in test_data]\n",
    "y_test=[sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "labels=list(crf.classes_)\n",
    "\n",
    "print('F1-score: {:0.4f}\\tAccuracy: {:0.4f}'.format(metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels),metrics.flat_accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "### Length of word\n",
    "Words of different parts-of-speech typically have different average length. For instance, nouns are usually longer than determiners.\n",
    "\n",
    "### Adverb \n",
    "Word ending with -ly are usually adverbs like happily, sadly, etc. This feature value is 1 when the word ends with -ly and its pos-tag is ADV, otherwise 0.\n",
    "\n",
    "### Verb\n",
    "The feature is 1 when the current word is the first word of the sentence, is a verb and the sentence ends with a question mark '?' , otherwise 0. Many interrogative sentences follow this pattern. For example, “Is this a sentence beginning with a verb?” \n",
    "\n",
    "### Adjective\n",
    "The feature is 1 when the current word is a NOUN and the previous word is an ADJECTIVE, otherwise 0. Intuitively, an adjective preceeds a noun which it describes. Like, \"A sleepy dog.\"\n",
    "\n",
    "### Noun\n",
    "The feature is 1 when the current word is a NOUN and its first letter is capital, otherwise 0. A proper noun begins with an uppercase letter.\n",
    "\n",
    "### Word\n",
    "The feature is the current word. It is similar to the emission matrix of HMM. A word usually has a fixed pos-tag irrespective of context. For e.g. \"the\" is always a Determiner.\n",
    "\n",
    "### is_first\n",
    "True if the current word is the first word of the sentence, False otherwise. Similar to start matrix of HMM.\n",
    "\n",
    "### is_last\n",
    "True if the current word is the last word of the sentence, False otherwise. Some pos-tags are more likely to end a sentence. So, this feature captures that.\n",
    "\n",
    "### is_capitalized\n",
    "True if the first letter of the word is capital, False otherwise. Certain pos-tags specifically associate with such words and some don't. For  e.g. proper nouns begin with a capital letter \n",
    "\n",
    "### is_all_caps\n",
    "True if the complete word is capitalized, False otherwise. For e.g I is a pronoun.\n",
    "\n",
    "### is_all_lower\n",
    "True if all letters of the word are in lowercase, False otherwise.\n",
    "\n",
    "### prefix-1 \n",
    "Equals 1-length prefix of the word. Similarly, we have 2 and 3 length prefixes as prefix-2 and prefix-3. Certain prefixes are common for a particular part-of-speech like im-, ana-, etc.\n",
    "\n",
    "### suffix-1 \n",
    "Equals 1-length suffix of the word. Similarly, we have 2 and 3 length suffixes as suffix-2 and suffix-3. Certain suffixes are common for a particular part-of-speech like -ly associates with Adverb, -ed associates with Verb, etc.\n",
    "\n",
    "### prev_word\n",
    "Previous word does affect the current word tag. For e.g. \"I asked him\", here \"asked\" is the previous word and it usually precedes a pronoun/noun.\n",
    "\n",
    "### next_word\n",
    "Similar to prev_word, next word also affects the current word tag.\n",
    "\n",
    "### has_hypen\n",
    "True if the current word has a hyphen, False otherwise. Specific pos words contain hyphen. \n",
    "\n",
    "### is_numeric\n",
    "True if the current word is actually a number, False otherwise. Models NUM pos-tag.\n",
    "\n",
    "### capitals_inside\n",
    "True if current word contains any capital letter after its first letter, False otherwise. \n",
    "\n",
    "### prev_tag\n",
    "POS tag of the preceding word affects the POS tag of the current word. It is similar to the transition matrix of the HMM.\n",
    "\n",
    "### curr_tag\n",
    "POS tag of the current word\n",
    "\n",
    "### next_tag\n",
    "POS tag of the next word also affects the POS tag of the current word.\n",
    "\n",
    "### number \n",
    "Feature is 1 if the first character of the word is digit and its POS tag is NUM, otherwise 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
